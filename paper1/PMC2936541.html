<p>Each sequenced sample was prepared according to the Illumina protocols. Briefly, one microgram of genomic DNA was fragmented by nebulization, the fragmented DNA was repaired, an &#8216;A&#8217; was ligated to the 3&#8242; end, Illumina adapters were then ligated to the fragments, and the sample was size selected aiming for a 350&#8211;400 base pair product. The size selected product was PCR amplified, and the final product was validated using the Agilent Bioanalyzer. Samples were then amplified on the flow cell and sequenced using the Genome Analyzer IIx, following the Illumina supplied protocols. The majority of sequence runs were paired end with 75 base reads. We aimed for 70&#8211;80 billion bases that passed the Illumina analysis filter per genome.</p><p>All sequencing data was produced and curated by the Genomic Analysis Facility, part of the Center for Human Genome Variation, at Duke University. After the sequencing reactions were complete, the Illumina analysis pipeline was used to process the raw sequencing data (Firecrest, Bustard and Gerald). The resulting Eland alignment was used only to estimate an error rate for each read/cluster. If a sequencing run/lane had an error rate above 2%, the run was considered a failure and the data was not used. The quality of the sequencing runs were also assessed by evaluating the percentage of clusters passing filter and the percentage of reads that align to the reference genome. For a typical run, over 70% of clusters pass filter and over 85% align. Any major deviations from these values would trigger further evaluation (average intensity, error graphs, etc) and likely lead to these runs/lanes not being used. The FASTQ files were then ready for the next alignment step.</p><p>Once the raw sequence data was curated, the reads were aligned to a reference genome (NCBI human genome assembly build 36) using the BWA software <xref ref-type="bibr" rid="pgen.1001111-Li1">[1]</xref>. Each alignment was assigned a mapping quality score by BWA, which is the <italic>Phred</italic>-scaled probability that the alignment is incorrect. The PCR amplification step will lead to the sequencing of identical DNA fragments. Not removing these PCR duplicates can lead to the miscalling of SNVs by overrepresentation of one allele. This is corrected by a quality control step to remove these potential PCR duplicates with SAMtools. Once all the reads have been aligned to the reference genome using BWA <xref ref-type="bibr" rid="pgen.1001111-Li1">[1]</xref>, we then used the SAMtools software <xref ref-type="bibr" rid="pgen.1001111-Li2">[2]</xref> to produce a consensus genotype for each genomic position. The consensus genotype is the genotype which has the highest probability of occurring after consideration of a number of factors <xref ref-type="bibr" rid="pgen.1001111-Li3">[37]</xref>. Each consensus genotype is then assigned a consensus quality, which is based on a <italic>Phred</italic>-scaled probability that the genotype call is incorrect. Single nucleotide variants (SNVs) and insertion/deletions (indels) were then identified based on differences between the consensus genotype and the reference allele at that position. SAMtools also assigns a <italic>Phred</italic>-scaled probability to each identified SNV/indel which indicates how likely it is that an inferred SNV/indel is identical to the reference. These SNVs and indels were then filtered by SAMtools' variation filter, changing only the maximum read depth parameter to call variants from its default value (100) to 10 million, to prevent the exclusion of SNVs and indels at a read depth greater than 100. The lists of SNVs/indels were then annotated in the SequenceVariantAnalyzer (SVA <xref ref-type="bibr" rid="pgen.1001111-Ge1">[23]</xref>). SVA was specifically designed to annotate the large number of identified SNVs/indels using a number of human genomic databases. In addition to looking at the SNVs/indels from the BWA alignment/SAMtools, we also predicted larger structural variation by developing an &#8220;Estimation by Read Depth with SNVs&#8221; or ERDS method, based on a Hidden Markov Model <xref ref-type="bibr" rid="pgen.1001111-Zhu1">[17]</xref>. This was an extension to the methods described in Bentley et al. <xref ref-type="bibr" rid="pgen.1001111-Bentley1">[3]</xref> and draft codes from Scally, A. <xref ref-type="bibr" rid="pgen.1001111-Scally1">[38]</xref>. Further details describing the analysis tools are presented in <xref ref-type="supplementary-material" rid="pgen.1001111.s018">Text S1</xref>.</p><p xmlns:ns0="http://www.w3.org/1999/xlink">The raw reads for a portion of the genomes used in this study are available on the NCBI sequence read archive, under study ID SRP001691 (<ext-link ext-link-type="uri" ns0:href="http://www.ncbi.nlm.nih.gov/sra/SRP001691">http://www.ncbi.nlm.nih.gov/sra/SRP001691</ext-link>). We do not have consent from the patients and/or permission from the Duke IRB to release the raw reads for several of the genomes used in this study.</p>